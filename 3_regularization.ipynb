{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAEACAYAAABPpeiSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3182c34d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Play with tf basic functions\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "with tf.Session():\n",
    "    t = tf.truncated_normal([64, 64])\n",
    "    y = t.eval()\n",
    "x = np.array([np.linspace(-1, 1, 64)])\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(x, y[0])\n",
    "plt.show()\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[1 1 1]\n",
      " [1 1 1]]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Play with tensor basics: dimesion, shape, reduce_xxx\n",
    "with tf.Session():\n",
    "    x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "    y = tf.reduce_sum(x)\n",
    "    print (x.get_shape())\n",
    "    print (x.eval())\n",
    "    print (y.eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is regularization for 1-hidden-layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    1: 8.000000 34.560000 37.680000 loss 493.913727\n",
      "Step  101: 78.000000 80.360000 87.540000 loss 82.235283\n",
      "Step  201: 83.000000 81.110000 88.430000 loss 39.831345\n",
      "Step  301: 80.000000 81.340000 88.450000 loss 35.849998\n",
      "Step  401: 86.000000 81.740000 89.320000 loss 28.311199\n",
      "Step  501: 84.500000 83.450000 89.880000 loss 22.590942\n",
      "Step  601: 82.000000 80.390000 86.380000 loss 31.901966\n",
      "Step  701: 83.000000 83.550000 89.700000 loss 20.806921\n",
      "Step  801: 83.500000 79.580000 86.130000 loss 25.482716\n",
      "Step  901: 82.500000 83.640000 90.310000 loss 23.285282\n",
      "Step 1000: 82.950000 90.220000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "hidden1_units = 1024\n",
    "hidden2_units = 256\n",
    "NUM_CLASSES = 10\n",
    "batch_size = 200\n",
    "TOTAL_STEPS = train_dataset.shape[0] // batch_size\n",
    "\n",
    "def evaluate(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "# TODO: this inference only fits for 2 layer NN model, should know of away to do it\n",
    "# more generically\n",
    "def inference2(d, w1, b1, w2, b2):\n",
    "    h1 = tf.nn.relu(tf.matmul(d, w1)+b1)\n",
    "    logits = tf.nn.relu(tf.matmul(h1, w2)+b2)\n",
    "    return tf.nn.softmax(logits)\n",
    "\n",
    "def inference3(d, w1, b1, w2, b2, w3, b3):\n",
    "    h1 = tf.nn.relu(tf.matmul(d, w1)+b1)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2)+b2)\n",
    "    logits = tf.nn.relu(tf.matmul(h2, w3)+b3)\n",
    "    return tf.nn.softmax(logits)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Define the tensorflow in some default graph\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, IMAGE_PIXELS))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    w1 = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units]), name='weights1')\n",
    "    b1 = tf.Variable(tf.zeros([hidden1_units]), name='biases1')\n",
    "    h1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "\n",
    "    w2 = tf.Variable(tf.truncated_normal([hidden1_units, NUM_CLASSES]), name='weights2')\n",
    "    b2 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases2')\n",
    "    logits = tf.matmul(h1, w2) + b2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    regularizers = (  tf.nn.l2_loss(w1) \n",
    "                    + tf.nn.l2_loss(b1) \n",
    "                    + tf.nn.l2_loss(w2) \n",
    "                    + tf.nn.l2_loss(b2))\n",
    "    loss += 1e-5 * regularizers\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    predicted_train = inference2(tf_train_dataset, w1, b1, w2, b2)\n",
    "    predicted_valid = inference2(valid_dataset, w1, b1, w2, b2)\n",
    "    predicted_test = inference2(test_dataset, w1, b1, w2, b2)\n",
    "    \n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for step in xrange(TOTAL_STEPS):\n",
    "        # Calculate the offset for the SGD training\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a batch slice\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare the feed dictionary\n",
    "        data_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, pt1 = sess.run([optimizer, loss, predicted_train], feed_dict=data_dict)\n",
    "        if step % 100 == 0:\n",
    "            print (\"Step %4d: %f %f %f loss %f\"%(step+1, evaluate(pt1, batch_labels),\n",
    "                                        evaluate(predicted_valid.eval(), valid_labels),\n",
    "                                        evaluate(predicted_test.eval(), test_labels),\n",
    "                                        l))\n",
    "    print (\"Step %4d: %f %f\"%(step+1, evaluate(predicted_valid.eval(), valid_labels),\n",
    "                                     evaluate(predicted_test.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is testing dropout function in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  [ 2.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session():\n",
    "    input1 = tf.constant([1.0, 1.0, 1.0, 1.0])\n",
    "    output1 = tf.nn.dropout(input1, 0.5)\n",
    "    result = output1.eval()\n",
    "    print (\"result: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    1: correct rate training:   9.38 validation:  37.65 test:  41.34, loss 2.400457\n",
      "Step  101: correct rate training:  81.25 validation:  82.62 test:  89.48, loss 0.567557\n",
      "Step  201: correct rate training:  87.50 validation:  82.12 test:  88.43, loss 0.484027\n",
      "Step  301: correct rate training:  88.28 validation:  84.77 test:  91.26, loss 0.462558\n",
      "Step  401: correct rate training:  86.72 validation:  85.52 test:  91.92, loss 0.472059\n",
      "Step  501: correct rate training:  77.34 validation:  85.63 test:  91.78, loss 0.614013\n",
      "Step  601: correct rate training:  87.50 validation:  86.73 test:  92.40, loss 0.377237\n",
      "Step  701: correct rate training:  89.06 validation:  86.22 test:  91.84, loss 0.442295\n",
      "Step  801: correct rate training:  90.62 validation:  87.02 test:  92.97, loss 0.302658\n",
      "Step  901: correct rate training:  85.16 validation:  86.74 test:  92.84, loss 0.513370\n",
      "Step 1001: correct rate training:  89.06 validation:  87.82 test:  93.66, loss 0.448975\n",
      "Step 1101: correct rate training:  85.94 validation:  87.35 test:  93.29, loss 0.455584\n",
      "Step 1201: correct rate training:  89.06 validation:  88.21 test:  93.93, loss 0.345750\n",
      "Step 1301: correct rate training:  93.75 validation:  87.77 test:  93.91, loss 0.332573\n",
      "Step 1401: correct rate training:  92.19 validation:  88.38 test:  94.00, loss 0.334442\n",
      "Step 1501: correct rate training:  87.50 validation:  87.99 test:  93.99, loss 0.398700\n",
      "Step 1601: correct rate training:  92.97 validation:  88.58 test:  93.98, loss 0.302535\n",
      "Step 1701: correct rate training:  87.50 validation:  88.72 test:  94.41, loss 0.355781\n",
      "Step 1801: correct rate training:  91.41 validation:  88.40 test:  94.05, loss 0.310061\n",
      "Step 1901: correct rate training:  85.94 validation:  86.76 test:  91.83, loss 0.397788\n",
      "Step 2001: correct rate training:  84.38 validation:  88.42 test:  94.33, loss 0.505386\n",
      "Step 2101: correct rate training:  90.62 validation:  88.49 test:  94.23, loss 0.273514\n",
      "Step 2201: correct rate training:  89.06 validation:  88.32 test:  93.94, loss 0.418080\n",
      "Step 2301: correct rate training:  87.50 validation:  88.06 test:  93.99, loss 0.358500\n",
      "Step 2401: correct rate training:  87.50 validation:  88.80 test:  94.44, loss 0.387011\n",
      "Step 2501: correct rate training:  86.72 validation:  88.94 test:  94.77, loss 0.429386\n",
      "Step 2601: correct rate training:  83.59 validation:  88.70 test:  94.65, loss 0.432192\n",
      "Step 2701: correct rate training:  90.62 validation:  88.77 test:  94.52, loss 0.256347\n",
      "Step 2801: correct rate training:  85.94 validation:  89.43 test:  94.64, loss 0.357005\n",
      "Step 2901: correct rate training:  89.84 validation:  88.89 test:  94.43, loss 0.331755\n",
      "Step 3001: correct rate training:  85.94 validation:  89.02 test:  94.59, loss 0.397283\n",
      "Step 3101: correct rate training:  89.84 validation:  89.39 test:  94.76, loss 0.241050\n",
      "Step 3201: correct rate training:  86.72 validation:  88.96 test:  94.45, loss 0.352206\n",
      "Step 3301: correct rate training:  89.06 validation:  89.60 test:  94.97, loss 0.313518\n",
      "Step 3401: correct rate training:  90.62 validation:  89.39 test:  94.67, loss 0.310508\n",
      "Step 3501: correct rate training:  92.19 validation:  89.13 test:  94.65, loss 0.259812\n",
      "Step 3601: correct rate training:  89.06 validation:  88.08 test:  92.81, loss 0.464735\n",
      "Step 3701: correct rate training:  90.62 validation:  89.57 test:  95.01, loss 0.234921\n",
      "Step 3801: correct rate training:  90.62 validation:  89.14 test:  94.76, loss 0.260808\n",
      "Step 3901: correct rate training:  89.06 validation:  89.47 test:  94.82, loss 0.277982\n",
      "Step 4001: correct rate training:  88.28 validation:  89.87 test:  94.99, loss 0.311796\n",
      "Step 4101: correct rate training:  92.97 validation:  89.12 test:  94.87, loss 0.242752\n",
      "Step 4201: correct rate training:  90.62 validation:  89.70 test:  94.99, loss 0.341621\n",
      "Step 4301: correct rate training:  90.62 validation:  89.55 test:  95.08, loss 0.304880\n",
      "Step 4401: correct rate training:  88.28 validation:  89.99 test:  95.14, loss 0.320053\n",
      "Step 4501: correct rate training:  89.84 validation:  89.53 test:  94.97, loss 0.285536\n",
      "Step 4601: correct rate training:  92.97 validation:  89.72 test:  95.09, loss 0.261425\n",
      "Step 4701: correct rate training:  95.31 validation:  89.39 test:  94.75, loss 0.201912\n",
      "Step 4801: correct rate training:  92.19 validation:  89.67 test:  95.24, loss 0.277281\n",
      "Step 4901: correct rate training:  92.97 validation:  89.77 test:  94.97, loss 0.195054\n",
      "Final step 5000: 89.730000 95.150000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "hidden1_units = 1024\n",
    "hidden2_units = 256\n",
    "hidden3_units = 128\n",
    "NUM_CLASSES = 10\n",
    "batch_size = 128\n",
    "TOTAL_STEPS = train_dataset.shape[0] // batch_size\n",
    "\n",
    "def evaluate(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "# TODO: this inference only fits for 2 layer NN model, should know of away to do it\n",
    "# more generically\n",
    "def inference2(d, w1, b1, w2, b2):\n",
    "    h1 = tf.nn.relu(tf.matmul(d, w1)+b1)\n",
    "    logits = tf.matmul(h1, w2)+b2\n",
    "    return tf.nn.softmax(logits)\n",
    "\n",
    "def inference3(d, w1, b1, w2, b2, w3, b3):\n",
    "    h1 = tf.nn.relu(tf.matmul(d, w1)+b1)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2)+b2)\n",
    "    logits = tf.matmul(h2, w3)+b3\n",
    "    return tf.nn.softmax(logits)\n",
    "\n",
    "def inference4(d, w1, b1, w2, b2, w3, b3, w4, b4):\n",
    "    hi1 = tf.nn.relu(tf.matmul(d, w1)+b1)\n",
    "    hi2 = tf.nn.relu(tf.matmul(hi1, w2)+b2)\n",
    "    hi3 = tf.nn.relu(tf.matmul(hi2, w3)+b3)\n",
    "    logits = tf.matmul(hi3, w4)+b4\n",
    "    return tf.nn.softmax(logits)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Define the tensorflow in some default graph\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, IMAGE_PIXELS))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Parameters\n",
    "    global_step = tf.Variable(0)\n",
    "    w1 = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units], stddev=np.sqrt(2.0 / IMAGE_PIXELS)))\n",
    "    b1 = tf.Variable(tf.zeros([hidden1_units]))\n",
    "\n",
    "    w2 = tf.Variable(tf.truncated_normal([hidden1_units, hidden2_units], stddev=np.sqrt(2.0 / hidden1_units)))\n",
    "    b2 = tf.Variable(tf.zeros([hidden2_units]))\n",
    "\n",
    "    w3 = tf.Variable(tf.truncated_normal([hidden2_units, hidden3_units], stddev=np.sqrt(2.0 / hidden2_units)))\n",
    "    b3 = tf.Variable(tf.zeros([hidden3_units]))\n",
    "\n",
    "    w4 = tf.Variable(tf.truncated_normal([hidden3_units, NUM_CLASSES], stddev=np.sqrt(2.0 / hidden3_units)))\n",
    "    b4 = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "\n",
    "    # Inference procedure\n",
    "    h1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, w3) + b3)\n",
    "    logits = tf.matmul(h3, w4) + b4\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    \n",
    "    # Optimize\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    predicted_train = tf.nn.softmax(logits)\n",
    "    \n",
    "    h1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, w1)+b1)\n",
    "    h2_valid = tf.nn.relu(tf.matmul(h1_valid, w2)+b2)\n",
    "    h3_valid = tf.nn.relu(tf.matmul(h2_valid, w3)+b3)\n",
    "    predicted_valid = tf.nn.softmax(tf.matmul(h3_valid, w4)+b4)\n",
    "\n",
    "    h1_test = tf.nn.relu(tf.matmul(tf_test_dataset, w1)+b1)\n",
    "    h2_test = tf.nn.relu(tf.matmul(h1_test, w2)+b2)\n",
    "    h3_test = tf.nn.relu(tf.matmul(h2_test, w3)+b3)\n",
    "    predicted_test = tf.nn.softmax(tf.matmul(h3_test, w4)+b4)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for step in xrange(5000):\n",
    "        # Calculate the offset for the SGD training\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a batch slice\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare the feed dictionary\n",
    "        data_dict = {\n",
    "                        tf_train_dataset : batch_data, \n",
    "                        tf_train_labels : batch_labels\n",
    "                    }\n",
    "        _, l, pt1 = sess.run([optimizer, loss, predicted_train], feed_dict=data_dict)\n",
    "        if step % 100 == 0:\n",
    "            print (\"Step %4d: correct rate training: %6.2f validation: %6.2f test: %6.2f, loss %8.6f\"%(\n",
    "                                        step+1, \n",
    "                                        evaluate(pt1, batch_labels),\n",
    "                                        evaluate(predicted_valid.eval(), valid_labels),\n",
    "                                        evaluate(predicted_test.eval(), test_labels),\n",
    "                                        l))\n",
    "    print (\"Final step %d: %f %f\"%(step+1, \n",
    "                                   evaluate(predicted_valid.eval(), valid_labels),\n",
    "                                   evaluate(predicted_test.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since the above code generate bad result, I will try to use somebody else's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step     0: 2.350111 11.7% 36.1%\n",
      "Minibatch loss at step   500: 0.595985 78.1% 85.6%\n",
      "Minibatch loss at step  1000: 0.443407 85.9% 87.8%\n",
      "Minibatch loss at step  1500: 0.370076 86.7% 88.1%\n",
      "Minibatch loss at step  2000: 0.545688 81.2% 87.8%\n",
      "Minibatch loss at step  2500: 0.340361 89.1% 89.2%\n",
      "Minibatch loss at step  3000: 0.412756 84.4% 89.2%\n",
      "Minibatch loss at step  3500: 0.335157 87.5% 89.3%\n",
      "Minibatch loss at step  4000: 0.332288 89.8% 89.7%\n",
      "Minibatch loss at step  4500: 0.261522 90.6% 90.3%\n",
      "Minibatch loss at step  5000: 0.277530 92.2% 90.3%\n",
      "Minibatch loss at step  5500: 0.210734 93.8% 89.7%\n",
      "Minibatch loss at step  6000: 0.196577 93.8% 90.2%\n",
      "Minibatch loss at step  6500: 0.191175 93.0% 90.0%\n",
      "Minibatch loss at step  7000: 0.111846 96.1% 90.4%\n",
      "Minibatch loss at step  7500: 0.221980 92.2% 90.5%\n",
      "Minibatch loss at step  8000: 0.201693 91.4% 90.4%\n",
      "Minibatch loss at step  8500: 0.173168 95.3% 90.8%\n",
      "Minibatch loss at step  9000: 0.200791 93.0% 90.6%\n",
      "Minibatch loss at step  9500: 0.106485 96.9% 90.9%\n",
      "Minibatch loss at step 10000: 0.179947 95.3% 91.0%\n",
      "Minibatch loss at step 10500: 0.122771 95.3% 90.6%\n",
      "Minibatch loss at step 11000: 0.117246 96.1% 91.1%\n",
      "Minibatch loss at step 11500: 0.062410 97.7% 91.1%\n",
      "Minibatch loss at step 12000: 0.146559 94.5% 90.8%\n",
      "Minibatch loss at step 12500: 0.146963 96.1% 91.0%\n",
      "Minibatch loss at step 13000: 0.112247 96.9% 91.2%\n",
      "Minibatch loss at step 13500: 0.036853 99.2% 91.2%\n",
      "Minibatch loss at step 14000: 0.098293 97.7% 91.0%\n",
      "Minibatch loss at step 14500: 0.141144 95.3% 91.2%\n",
      "Minibatch loss at step 15000: 0.055207 98.4% 91.2%\n",
      "Minibatch loss at step 15500: 0.012294 100.0% 91.2%\n",
      "Minibatch loss at step 16000: 0.060375 98.4% 91.2%\n",
      "Minibatch loss at step 16500: 0.119906 95.3% 91.4%\n",
      "Minibatch loss at step 17000: 0.050617 97.7% 91.4%\n",
      "Minibatch loss at step 17500: 0.047074 98.4% 91.1%\n",
      "Minibatch loss at step 18000: 0.031107 99.2% 91.2%\n",
      "Test accuracy: 96.3%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes1], \n",
    "                                               stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "    weights4 = tf.Variable(tf.truncated_normal([num_hidden_nodes3, num_labels], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "    logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)\n",
    "    \n",
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %5d: %f %.1f%% %.1f%%\" % (step, l, \n",
    "                                                                    evaluate(predictions, batch_labels),\n",
    "                                                                   evaluate(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: %.1f%%\" % evaluate(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
